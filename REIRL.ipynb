{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def feature_expectations(rewards, gamma):\n",
    "    discount_factor_timestep = np.power(gamma * np.ones(rewards.shape[1]),\n",
    "                                        range(rewards.shape[1]))\n",
    "    discounted_return = discount_factor_timestep[np.newaxis, :, np.newaxis] * rewards\n",
    "    reward_est_timestep = np.sum(discounted_return, axis=1)\n",
    "    return reward_est_timestep\n",
    "\n",
    "class RelativeEntropyIRL(object):\n",
    "    eps = 1e-24\n",
    "\n",
    "    def __init__(self,\n",
    "                 reward_features,\n",
    "                 reward_random,\n",
    "                 gamma,\n",
    "                 horizon,\n",
    "                 trajectories_expert=None,\n",
    "                 trajectories_random=None,\n",
    "                 n_states=None,\n",
    "                 n_actions=None,\n",
    "                 learning_rate=0.01,\n",
    "                 max_iter=100,\n",
    "                 type_='state',\n",
    "                 gradient_method='linear',\n",
    "                 evaluation_horizon=100):\n",
    "\n",
    "        # transition model: tensor (n_states, n_actions, n_states)\n",
    "\n",
    "        self.reward_features = reward_features\n",
    "        self.reward_random = reward_random\n",
    "        self.trajectories_expert = trajectories_expert\n",
    "        self.trajectories_random = trajectories_random\n",
    "        self.gamma = gamma\n",
    "        self.horizon = horizon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        if not type_ in ['state', 'state-action']:\n",
    "            raise ValueError()\n",
    "        self.type_ = type_\n",
    "\n",
    "        if not gradient_method in ['linear', 'exponentiated']:\n",
    "            raise ValueError()\n",
    "        self.gradient_method = gradient_method\n",
    "\n",
    "        self.evaluation_horizon = evaluation_horizon\n",
    "\n",
    "        self.n_states, self.n_actions = n_states, n_actions\n",
    "        self.n_features = reward_features.shape[2]\n",
    "\n",
    "    def fit(self, verbose=False):\n",
    "\n",
    "        #Compute features expectations\n",
    "        expert_feature_expectations = feature_expectations(self.reward_features, self.gamma)\n",
    "        random_feature_expectations = feature_expectations(self.reward_random, self.gamma)\n",
    "        return reirl(expert_feature_expectations, random_feature_expectations, max_iter=self.max_iter,\n",
    "                     learning_rate=self.learning_rate, verbose=verbose)\n",
    "\n",
    "        # expert_feature_expectations\n",
    "        # n_random_trajectories = self.reward_random.shape[0]\n",
    "        # importance_sampling = np.zeros(n_random_trajectories)\n",
    "        #\n",
    "        # #Weights initialization\n",
    "        # w = np.ones(self.n_features) / self.n_features\n",
    "        # #Gradient descent\n",
    "        # for i in range(self.max_iter):\n",
    "        #     if verbose:\n",
    "        #         print('Iteration %s/%s' % (i + 1, self.max_iter))\n",
    "        #\n",
    "        #     for j in range(n_random_trajectories):\n",
    "        #         importance_sampling[j] = np.dot(random_feature_expectations[j], w)\n",
    "        #     importance_sampling -= np.max(importance_sampling)\n",
    "        #     importance_sampling = np.exp(importance_sampling)/np.sum(np.exp(importance_sampling), axis=0)\n",
    "        #     weighted_sum = np.sum(\n",
    "        #         np.multiply(np.array([importance_sampling, ] * random_feature_expectations.shape[1]).T,\n",
    "        #                     random_feature_expectations), axis=0)\n",
    "        #\n",
    "        #     w += self.learning_rate * (expert_feature_expectations_mean - weighted_sum)\n",
    "        #     # One weird trick to ensure that the weights don't blow up the objective.\n",
    "        #     w = w / np.linalg.norm(w, keepdims=True)\n",
    "        # return w\n",
    "\n",
    "\n",
    "def reirl(expert_feature_expectations, random_feature_expectations, max_iter=100, learning_rate=0.01, verbose=False):\n",
    "    # Compute features expectations\n",
    "    expert_feature_expectations_mean = np.mean(expert_feature_expectations, axis=0)\n",
    "\n",
    "    n_random_trajectories = int(len(random_feature_expectations))\n",
    "    importance_sampling = np.zeros(n_random_trajectories)\n",
    "\n",
    "    # Weights initialization\n",
    "    n_features = expert_feature_expectations_mean.shape[-1]\n",
    "    w = np.ones(n_features) / n_features\n",
    "\n",
    "    # Gradient descent\n",
    "    for i in range(max_iter):\n",
    "\n",
    "        if verbose:\n",
    "            print('Iteration %s/%s' % (i + 1, max_iter))\n",
    "\n",
    "        for j in range(n_random_trajectories):\n",
    "            importance_sampling[j] = np.exp(np.dot(random_feature_expectations[j], w))\n",
    "        importance_sampling /= np.sum(importance_sampling, axis=0)\n",
    "        weighted_sum = np.sum(np.multiply(np.array([importance_sampling, ] * random_feature_expectations.shape[1]).T,\n",
    "                                          random_feature_expectations), axis=0)\n",
    "\n",
    "        w += learning_rate * (expert_feature_expectations_mean - weighted_sum)\n",
    "\n",
    "        # One weird trick to ensure that the weights don't blow up the objective.\n",
    "        w = w / np.linalg.norm(w, keepdims=True)\n",
    "\n",
    "    w /= np.linalg.norm(w, ord=1, keepdims=True)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import sleep\n",
    "from envs.continuous_gridword import GridWorldAction\n",
    "from estimators.gradient_descent import Adam\n",
    "import argparse\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "from plot_gridworld import plot_grid\n",
    "\n",
    "\n",
    "def create_batch_trajectories(env, batch_size, len_trajectories, param, variance, render=False):\n",
    "    state_dim = np.prod(env.observation_space.shape)\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    states = np.zeros((batch_size, len_trajectories, state_dim))\n",
    "    actions = np.zeros((batch_size, len_trajectories, action_dim))\n",
    "    rewards = np.zeros((batch_size, len_trajectories))\n",
    "    mask = np.ones((batch_size, len_trajectories))\n",
    "    reward_features = np.zeros((batch_size, len_trajectories, 3))\n",
    "\n",
    "    for batch in range(batch_size):\n",
    "        state = env.reset(rbf=True)\n",
    "\n",
    "        if render and batch == 0:\n",
    "            env._render()\n",
    "            sleep(0.1)\n",
    "\n",
    "        for t in range(len_trajectories):\n",
    "            action = np.random.multivariate_normal(np.dot(param.T, state), np.eye(action_dim) * variance)\n",
    "            next_state, reward, done, info = env.step(action, rbf=True)\n",
    "\n",
    "            if render and batch == 0:  # render:\n",
    "                env._render()\n",
    "                sleep(0.1)\n",
    "\n",
    "            # print(state.shape)\n",
    "            states[batch, t] = state\n",
    "            actions[batch, t] = action\n",
    "            rewards[batch, t] = reward\n",
    "            reward_features[batch, t] = info['features']\n",
    "\n",
    "            if done:\n",
    "                states[batch, t + 1:] = state\n",
    "                actions[batch, t + 1:] = action\n",
    "                reward_features[batch, t + 1:] = info['features'][:3]\n",
    "                mask[batch, t + 1:] = 0.\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return states, actions, rewards, reward_features, mask\n",
    "\n",
    "\n",
    "def gradient_est(param, batch_size, len_trajectories, states, actions, var_policy):\n",
    "    gradients = np.zeros((batch_size, len_trajectories, param.shape[0], param.shape[1]))\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for t in range(len_trajectories):\n",
    "            action = actions[b, t]\n",
    "            state = states[b, t]\n",
    "            mean = np.dot(param.T, state)\n",
    "            gradients[b, t, :, :] = np.outer(action - mean, state).T / var_policy\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def policy_gradient_est(param, batch_size, len_trajectories, states, actions, rewards, reward_features, mask, gamma,\n",
    "                        return_mean=True, return_ravel=False):\n",
    "    if reward_features is not None:\n",
    "        rewards = reward_features\n",
    "    else:\n",
    "        rewards = rewards[:, :, None]\n",
    "\n",
    "    discount_factor_timestep = gamma ** np.arange(len_trajectories)\n",
    "    discounted_reward = discount_factor_timestep[None, :, None] * rewards * mask[:, :, None]  # (N, T, k)\n",
    "\n",
    "    gradients = gradient_est(param, batch_size, len_trajectories, states, actions, var_policy)  # (N, T, q, d)\n",
    "    gradient_timestep = np.cumsum(gradients * mask[:, :, None, None], axis=1)  # (N, T, q, d)\n",
    "\n",
    "    baseline_den = np.mean(gradient_timestep ** 2, axis=0)[:, :, :, None] + 1e-24  # (T, q, d, 1)\n",
    "    baseline_num = np.mean(gradient_timestep[:, :, :, :, None] ** 2 * discounted_reward[:, :, None, None, :],\n",
    "                           axis=0)  # (T, q, d, k)\n",
    "    baseline = baseline_num / baseline_den\n",
    "\n",
    "    gradient_per_episode = np.sum(gradient_timestep[:, :, :, :, None] * (discounted_reward[:, :, None, None, :]\n",
    "                                                                         - baseline[None, :, :, :, :]),\n",
    "                                  axis=1)  # (q, d, k)\n",
    "\n",
    "    if return_mean:\n",
    "        gradient = np.mean(gradient_per_episode, axis=0)\n",
    "        if return_ravel:\n",
    "            gradient = gradient.reshape(-1, gradient.shape[-1])\n",
    "    else:\n",
    "        gradient = gradient_per_episode\n",
    "        if return_ravel:\n",
    "            gradient = gradient.reshape(gradient.shape[0], -1, gradient.shape[-1])\n",
    "\n",
    "    return gradient.squeeze()\n",
    "\n",
    "\n",
    "def gpomdp(env, num_batch, batch_size, len_trajectories, initial_param, gamma, var_policy, verbose=False, render=False):\n",
    "    param = np.array(initial_param)\n",
    "    results = np.zeros(num_batch)\n",
    "\n",
    "    discount_factor_timestep = gamma ** np.arange(len_trajectories)\n",
    "\n",
    "    gradient = np.zeros_like(param)\n",
    "    rewards__ = np.zeros(num_batch)\n",
    "    gradients__ = np.zeros(num_batch)\n",
    "    lens__ = np.zeros(num_batch)\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.05, ascent=True)\n",
    "    optimizer.initialize(param)\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        if i > 0:\n",
    "            param = optimizer.update(gradient)\n",
    "            # param += 0.05*gradient\n",
    "        states, actions, rewards, _, mask = create_batch_trajectories(env, batch_size, len_trajectories, param,\n",
    "                                                                      var_policy, render=render)\n",
    "\n",
    "        gradient = policy_gradient_est(param, batch_size, len_trajectories, states, actions, rewards, None, mask, gamma)\n",
    "\n",
    "        results[i] = np.mean(np.sum(discount_factor_timestep[None, :] * rewards * mask, axis=1), axis=0)\n",
    "\n",
    "        if verbose:\n",
    "            gradients__[i] = np.linalg.norm(gradient.ravel())\n",
    "            rewards__[i] = np.mean(np.sum(rewards, axis=1))\n",
    "            lens__[i] = np.mean(np.sum(mask, axis=1))\n",
    "            print('Ite %s - Grad %s - Rewards %s - Lens %s' % (i, gradients__[i], rewards__[i], lens__[i]))\n",
    "\n",
    "    return param, results, states, rewards__, gradients__\n",
    "\n",
    "\n",
    "def compute_feature_expectations(len_trajectories, reward_features, mask, gamma):\n",
    "    discount_factor_timestep = gamma ** np.arange(len_trajectories)\n",
    "    return np.sum(discount_factor_timestep[None, :, None] * mask[:, :, None] * reward_features, axis=1)\n",
    "\n",
    "\n",
    "def discretize_actions(actions, action_max, action_min, n_bins_per_dim):\n",
    "    actions = np.clip(actions, action_min, action_max)\n",
    "    dims = len(action_max)\n",
    "    discretized_actions = np.zeros(actions.shape[:2])\n",
    "\n",
    "    for i in range(dims):\n",
    "        bins = np.linspace(action_min[i] - 1e-12, action_max[i] + 1e-12, n_bins_per_dim + 1)\n",
    "        indices = np.digitize(actions[:, :, i].ravel(), bins).reshape(actions[:, :, i].shape) - 1\n",
    "        assert np.max(indices) <= n_bins_per_dim - 1 and np.min(indices) >= 0\n",
    "        discretized_actions += indices * n_bins_per_dim ** i\n",
    "    assert np.min(discretized_actions) >= 0 and np.max(discretized_actions) <= n_bins_per_dim ** dims - 1\n",
    "    return discretized_actions.astype(int)\n",
    "\n",
    "\n",
    "def _perform_irl(i, N_traj_irl, n_samples_irl, horizon, fail_prob, save_gradients, param, var_policy, save_path):\n",
    "    np.random.seed(i)\n",
    "    res = np.zeros((len(n_samples_irl), 1))\n",
    "    weights_diff = np.zeros((len(n_samples_irl), 1))\n",
    "    env = GridWorldAction(shape=shape, rew_weights=rew_weights,\n",
    "                          randomized_initial=True, horizon=horizon,\n",
    "                          n_bases=n_basis, fail_prob=fail_prob,\n",
    "                          border_width=1)\n",
    "\n",
    "    print(\"\\nIRL Experiment %s\" % i)\n",
    "    print(\"Collecting Trajectories and Computing Gradients...\")\n",
    "\n",
    "    env_irl = GridWorldAction(shape=shape, rew_weights=[0., 0., 0.],\n",
    "                              randomized_initial=True, horizon=horizon,\n",
    "                              n_bases=n_basis, fail_prob=fail_prob,\n",
    "                              border_width=1)\n",
    "\n",
    "    states, actions, _, reward_features, mask = create_batch_trajectories(env_irl, batch_size=N_traj_irl,\n",
    "                                                                          len_trajectories=horizon, param=param,\n",
    "                                                                          variance=var_policy)\n",
    "\n",
    "    discretized_actions = discretize_actions(actions, env.action_high, env.action_low, n_bins_per_dim=3)\n",
    "\n",
    "    estimated_gradients = policy_gradient_est(param, batch_size=N_traj_irl, len_trajectories=horizon,\n",
    "                                              states=states, actions=actions, rewards=None,\n",
    "                                              reward_features=reward_features, mask=mask, gamma=gamma,\n",
    "                                              return_mean=False, return_ravel=True)\n",
    "\n",
    "    feature_exp = compute_feature_expectations(horizon, reward_features, mask, gamma)\n",
    "\n",
    "    if save_gradients:\n",
    "        np.save(save_path + \"/gridworld_grads_%s_%s_%s_%s.npy\" % tuple(rew_weights + [i]), estimated_gradients)\n",
    "\n",
    "    # Collect trajectories for REIRL\n",
    "    _, _, _, reward_features_random, mask = create_batch_trajectories(env_irl, batch_size=N_traj_irl,\n",
    "                                                                      len_trajectories=horizon,\n",
    "                                                                      param=np.zeros_like(param),\n",
    "                                                                      variance=1)\n",
    "\n",
    "    feature_exp_random = compute_feature_expectations(horizon, reward_features_random, mask, gamma)\n",
    "\n",
    "    for j, n in enumerate(n_samples_irl):\n",
    "        print(\"\\n\\tIRL Episodes: %s\" % n)\n",
    "\n",
    "        # REIRL\n",
    "        print(\"\\tSolving REIRL\")\n",
    "        weights_reirl = reirl(feature_exp[:n], feature_exp_random[:n])\n",
    "\n",
    "        weights_diff[j,0] = np.linalg.norm(weights_reirl - rew_weights)\n",
    "\n",
    "        if train_after_irl:\n",
    "\n",
    "            env_train_ = GridWorldAction(shape=shape, rew_weights=weights_reirl,\n",
    "                                         randomized_initial=True, horizon=horizon,\n",
    "                                         n_bases=n_basis, fail_prob=fail_prob,\n",
    "                                         border_width=1)\n",
    "\n",
    "            params_, _, _, _, _ = gpomdp(env_train_, num_batch=50, batch_size=100, len_trajectories=horizon,\n",
    "                                         initial_param=np.zeros((np.prod(n_basis), 2)),\n",
    "                                         gamma=gamma, var_policy=var_policy, verbose=False)\n",
    "\n",
    "            _, results_, _, _, _ = gpomdp(env, num_batch=1, batch_size=100, len_trajectories=horizon,\n",
    "                                          initial_param=params_,\n",
    "                                          gamma=gamma, var_policy=var_policy, verbose=False)\n",
    "\n",
    "            print(\"\\t\\tPerformance %s: %s\" % (\"REIRL\", results_[0]))\n",
    "\n",
    "            res[j, 0] = results_[0]\n",
    "\n",
    "    return res, weights_diff, \"REILR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ite 0 - Grad 27.24258327133451 - Rewards -47.90683168316832 - Lens 81.98\n",
      "Ite 1 - Grad 9.579038297003978 - Rewards -21.044158415841583 - Lens 60.24\n",
      "Ite 2 - Grad 10.128501375404614 - Rewards -14.262079207920788 - Lens 61.68\n",
      "Ite 3 - Grad 3.3975584045836547 - Rewards -12.298217821782178 - Lens 64.03\n",
      "Ite 4 - Grad 4.447072600406869 - Rewards -11.240594059405941 - Lens 57.9\n",
      "Ite 5 - Grad 5.46232270480162 - Rewards -12.778415841584156 - Lens 57.84\n",
      "Ite 6 - Grad 4.037892629568113 - Rewards -11.727623762376236 - Lens 54.56\n",
      "Ite 7 - Grad 4.090615550315577 - Rewards -11.784950495049502 - Lens 53.21\n",
      "Ite 8 - Grad 5.574128676104215 - Rewards -13.039900990099008 - Lens 58.26\n",
      "Ite 9 - Grad 3.5608958446637557 - Rewards -12.144653465346533 - Lens 56.53\n",
      "Ite 10 - Grad 3.211530749801284 - Rewards -8.177326732673269 - Lens 42.98\n",
      "Ite 11 - Grad 4.591118289896571 - Rewards -10.277722772277228 - Lens 51.49\n",
      "Ite 12 - Grad 3.180819967807844 - Rewards -9.536633663366336 - Lens 51.88\n",
      "Ite 13 - Grad 3.850258191567362 - Rewards -9.572079207920792 - Lens 57.54\n",
      "Ite 14 - Grad 4.843701496534099 - Rewards -9.67217821782178 - Lens 49.41\n",
      "Ite 15 - Grad 3.7874694546746666 - Rewards -10.311782178217822 - Lens 55.68\n",
      "Ite 16 - Grad 3.672731306491283 - Rewards -10.325049504950494 - Lens 57.18\n",
      "Ite 17 - Grad 3.712901884163886 - Rewards -8.26217821782178 - Lens 43.32\n",
      "Ite 18 - Grad 3.249776549324436 - Rewards -7.845940594059405 - Lens 39.42\n",
      "Ite 19 - Grad 4.090812561083256 - Rewards -9.645841584158415 - Lens 47.76\n",
      "Ite 20 - Grad 4.202404420160622 - Rewards -11.166138613861383 - Lens 51.23\n",
      "Ite 21 - Grad 3.841684478649334 - Rewards -8.59970297029703 - Lens 43.5\n",
      "Ite 22 - Grad 5.25306353441012 - Rewards -10.153960396039604 - Lens 48.35\n",
      "Ite 23 - Grad 3.7712170269949645 - Rewards -8.66049504950495 - Lens 43.2\n",
      "Ite 24 - Grad 5.957208508234631 - Rewards -9.832871287128713 - Lens 50.75\n",
      "Ite 25 - Grad 4.053795797109041 - Rewards -8.629405940594058 - Lens 45.82\n",
      "Ite 26 - Grad 4.930591693041075 - Rewards -7.956534653465346 - Lens 35.45\n",
      "Ite 27 - Grad 4.977643980535804 - Rewards -7.286237623762377 - Lens 31.61\n",
      "Ite 28 - Grad 2.9438153933167284 - Rewards -5.23920792079208 - Lens 24.98\n",
      "Ite 29 - Grad 2.8502631777986664 - Rewards -5.087128712871287 - Lens 26.44\n",
      "Ite 30 - Grad 2.6688038546532047 - Rewards -4.539405940594059 - Lens 21.38\n",
      "Ite 31 - Grad 2.8467292734356757 - Rewards -5.057227722772278 - Lens 22.31\n",
      "Ite 32 - Grad 2.3291199390193364 - Rewards -5.872970297029703 - Lens 25.79\n",
      "Ite 33 - Grad 3.3464461633361857 - Rewards -5.637227722772278 - Lens 24.87\n",
      "Ite 34 - Grad 2.1634614117487443 - Rewards -4.564158415841584 - Lens 18.48\n",
      "Ite 35 - Grad 3.2166992634499714 - Rewards -5.180396039603959 - Lens 28.69\n",
      "Ite 36 - Grad 1.1159578134571668 - Rewards -4.0212871287128715 - Lens 21.91\n",
      "Ite 37 - Grad 1.6561897150874416 - Rewards -3.5414851485148517 - Lens 18.21\n",
      "Ite 38 - Grad 2.5502860840145725 - Rewards -3.933663366336633 - Lens 28.41\n",
      "Ite 39 - Grad 1.44887883234537 - Rewards -3.7338613861386136 - Lens 23.85\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "\n",
      "IRL Experiment 0\n",
      "Collecting Trajectories and Computing Gradients...\n",
      "\n",
      "\tIRL Episodes: 2\n",
      "\tSolving REIRL\n",
      "\t\tPerformance REIRL: -10.102466412936348\n",
      "\n",
      "\tIRL Episodes: 5\n",
      "\tSolving REIRL\n",
      "\t\tPerformance REIRL: -3.8907508403242184\n",
      "\n",
      "\tIRL Episodes: 10\n",
      "\tSolving REIRL\n",
      "\t\tPerformance REIRL: -8.350833568207277\n",
      "\n",
      "\tIRL Episodes: 20\n",
      "\tSolving REIRL\n",
      "\t\tPerformance REIRL: -10.510861993291792\n",
      "\n",
      "\tIRL Episodes: 50\n",
      "\tSolving REIRL\n",
      "\t\tPerformance REIRL: -8.373875219972406\n",
      "\n",
      "\tIRL Episodes: 100\n",
      "\tSolving REIRL\n",
      "\t\tPerformance REIRL: -8.042798503341988\n",
      "\n",
      "\tIRL Episodes: 200\n",
      "\tSolving REIRL\n",
      "\t\tPerformance REIRL: -5.457605198994746\n",
      "\n",
      "\tIRL Episodes: 500\n",
      "\tSolving REIRL\n",
      "\t\tPerformance REIRL: -7.530896126662681\n",
      "\n",
      "\tIRL Episodes: 1000\n",
      "\tSolving REIRL\n",
      "\t\tPerformance REIRL: -3.2226827152340873\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 52.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 52.6min finished\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-587bb6ebeea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mplot_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples_irl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\sigma-girl-MIIRL\\plot_gridworld.py\u001b[0m in \u001b[0;36mplot_grid\u001b[1;34m(n_samples_irl, res, all_weights, alg_names, save_path, fix_re_irl)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malg_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mmean_to_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres_mean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mstd_to_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres_std\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m're_irl'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfix_re_irl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#parser.add_argument('--horizon', type=int, default=100, help='length of the episodes')\n",
    "#parser.add_argument('--gamma', type=float, default=0.99, help='discount factor')\n",
    "#parser.add_argument('--var_policy', type=float, default=0.1, help='variance of the policy')\n",
    "#parser.add_argument('--shape', type=int, nargs='+', default=[], help='shape of gird')\n",
    "#parser.add_argument('--n_basis', type=int, nargs='+', default=[], help='number of rbf basis for the state '\n",
    "#                                                                       'representation')\n",
    "#parser.add_argument('--fail_prob', type=float, default=0.1, help='stochasticity of the environment')\n",
    "#parser.add_argument('--load_policy', action='store_true', help='load a pretrained policy')\n",
    "#parser.add_argument('--load_path', type=str, default='data/gridworld_single', help='path to model to load')\n",
    "#parser.add_argument('--save_policy', action='store_true', help='save the trained policy')\n",
    "#parser.add_argument('--save_path', type=str, default='data/gridworld_single', help='path to save the model')\n",
    "#parser.add_argument('--save_gradients', action='store_true', help='save the computed gradients')\n",
    "#parser.add_argument('--train_after_irl', action='store_true', help='train with the computed rewards')\n",
    "#parser.add_argument('--n_experiments', type=int, default=10, help='number of experiments to perform')\n",
    "#parser.add_argument('--n_jobs', type=int, default=1, help='number of parallel jobs')\n",
    "#parser.add_argument('--render_policy', action='store_true', help='render the interaction with the environment')\n",
    "#parser.add_argument('--plot_results', action='store_true', help='plot the results')\n",
    "horizon = 100\n",
    "gamma =0.99\n",
    "var_policy=0.1\n",
    "shape=[5,5]\n",
    "n_basis=[10,10]\n",
    "fail_prob=0.1\n",
    "load_policy=False\n",
    "save_policy=False\n",
    "save_path='data/gridworld_single'\n",
    "train_after_irl=True\n",
    "n_experiments=1\n",
    "n_jobs=1\n",
    "render_policy=True\n",
    "plot_results=True\n",
    "save_gradients=False\n",
    "\n",
    "n_samples_irl = [2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "N_traj_irl = max(n_samples_irl)\n",
    "\n",
    "n_experiments = n_experiments\n",
    "horizon = horizon\n",
    "gamma = gamma\n",
    "var_policy = var_policy\n",
    "fail_prob = fail_prob\n",
    "\n",
    "load_policy = load_policy\n",
    "save_policy = save_policy\n",
    "save_gradients = save_gradients\n",
    "train_after_irl = train_after_irl\n",
    "\n",
    "rew_weights = np.array([1., 100., 0.])\n",
    "rew_weights /= np.sum(rew_weights)\n",
    "rew_weights = rew_weights.tolist()\n",
    "\n",
    "res = np.zeros((n_experiments, len(n_samples_irl), 1))\n",
    "all_weights = np.zeros((n_experiments, len(n_samples_irl), 1))\n",
    "\n",
    "env = GridWorldAction(shape=shape, rew_weights=rew_weights,\n",
    "                      randomized_initial=True, horizon=horizon,\n",
    "                      n_bases=n_basis, fail_prob=fail_prob,\n",
    "                      border_width=1)\n",
    "\n",
    "if load_policy:\n",
    "    print(\"Loading policy...\")\n",
    "    param = np.load(load_path + \"/gridworld_param_%s_%s_%s_fail=%s.npy\" % tuple(rew_weights + [fail_prob]))\n",
    "else:\n",
    "    # Train expert\n",
    "\n",
    "    param, _, _, _, _ = gpomdp(env, num_batch=40, batch_size=100, len_trajectories=horizon,\n",
    "                               initial_param=np.zeros((np.prod(n_basis), 2)),\n",
    "                               gamma=gamma, var_policy=var_policy, verbose=True)\n",
    "\n",
    "    if save_policy:\n",
    "        np.save(save_path + \"/gridworld_param_%s_%s_%s_fail=%s.npy\" % tuple(rew_weights + [fail_prob]), param)\n",
    "\n",
    "if render_policy:\n",
    "    _, _, _, _, _ = gpomdp(env, num_batch=3, batch_size=1, len_trajectories=horizon,\n",
    "                           initial_param=param,\n",
    "                           gamma=gamma, var_policy=0., verbose=False, render=True)\n",
    "\n",
    "_, results, _, _, _ = gpomdp(env, num_batch=1, batch_size=500, len_trajectories=horizon,\n",
    "                             initial_param=param,\n",
    "                             gamma=gamma, var_policy=var_policy, verbose=False, render=False)\n",
    "\n",
    "all_args = N_traj_irl, n_samples_irl, horizon, fail_prob, save_gradients, param, var_policy, save_path\n",
    "all_results = Parallel(n_jobs=n_jobs, verbose=51)(\n",
    "    delayed(_perform_irl)(i, *all_args) for i in range(n_experiments))\n",
    "\n",
    "for i in range(n_experiments):\n",
    "    res[i], all_weights[i], agents = all_results[i]\n",
    "\n",
    "np.save(save_path + '/gridworld_res_irl_%s.npy' % fail_prob, res)\n",
    "np.save(save_path + '/gridworld_res_irl_w_%s.npy' % fail_prob, all_weights)\n",
    "\n",
    "if plot_results:\n",
    "    plot_grid(n_samples_irl, res, all_weights, agents, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-10.10246641],\n",
       "        [ -3.89075084],\n",
       "        [ -8.35083357],\n",
       "        [-10.51086199],\n",
       "        [ -8.37387522],\n",
       "        [ -8.0427985 ],\n",
       "        [ -5.4576052 ],\n",
       "        [ -7.53089613],\n",
       "        [ -3.22268272]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl",
   "language": "python",
   "name": "irl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
